# LangSmith Offline Evaluation Setup

## Overview

Complete offline evaluation system for DR using LangSmith, with support for both agent-level (end-to-end) and component-level (isolated LLM) evaluation.

## What Was Implemented

### 1. Ground Truth Directory Structure
```
ground_truth/
├── README.md           # Documentation on workflow and file format
└── *.json             # Ground truth examples (generated)
```

### 2. CLI Commands (`dr eval`)

All evaluation commands are now available under `dr eval`:

#### Generate Mock Ground Truth
```bash
dr eval generate-ground-truth --num 5
```

Options:
- `--num N`: Number of examples to generate (default: 5)
- `--ticker TICKER`: Specific ticker (default: random selection)
- `--output-dir DIR`: Output directory (default: ground_truth/)
- `--quality-tier [mock|silver|gold]`: Quality tier (default: mock)
- `--days-back N`: Days back to look for historical data (default: 7)

#### Upload Dataset to LangSmith
```bash
dr eval upload-dataset --from ground_truth/ --name dr-eval-v1 --type agent
```

Options:
- `--from DIR`: Source directory with ground truth JSON files
- `--name NAME`: Dataset name in LangSmith
- `--type [agent|component]`: Dataset type
- `--description TEXT`: Optional description

#### Run Agent-Level Evaluation
```bash
# Remote (LangSmith)
dr eval agent --dataset dr-eval-v1

# Local (no LangSmith needed)
dr eval agent --dataset ground_truth/ --local
```

Options:
- `--dataset NAME`: Dataset name (LangSmith dataset name or local directory path)
- `--experiment NAME`: Optional experiment name
- `--local`: Run evaluation locally without LangSmith (saves results to JSON)

#### Run Component-Level Evaluation
```bash
# Remote (LangSmith)
dr eval component report-generation --dataset dr-eval-v1

# Local (no LangSmith needed)
dr eval component report-generation --dataset ground_truth/ --local
```

Options:
- `component_name`: Component to evaluate (currently: `report-generation`)
- `--dataset NAME`: Dataset name (LangSmith dataset name or local directory path)
- `--experiment NAME`: Optional experiment name
- `--local`: Run evaluation locally without LangSmith (saves results to JSON)

#### Discoverability Commands

**List Components**
```bash
dr eval list-components
```
Shows all available components for evaluation with descriptions and usage examples.

**List Datasets**
```bash
dr eval list-datasets              # List both local and remote
dr eval list-datasets --local      # Local only
dr eval list-datasets --remote     # Remote (LangSmith) only
```
Lists available datasets (local ground truth and LangSmith datasets).

**Status**
```bash
dr eval status
```
Shows evaluation system status overview: available components, local ground truth, evaluation results, and LangSmith connection status.

**Describe**
```bash
dr eval describe component report-generation
dr eval describe dataset ground_truth/
dr eval describe dataset my-dataset-v1
```
Shows detailed information about a component or dataset.

### 3. Evaluation Runners

**Agent-Level** (`scripts/eval_agent.py`):
- Runs full TickerAnalysisAgent workflow
- Compares against ground truth using all 7 evaluators
- Logs results to LangSmith

**Component-Level** (`scripts/eval_component.py`):
- Runs isolated report generation (no data fetching)
- Uses pre-fetched data from dataset inputs
- Same 7 evaluators for comparison

**Local Evaluation** (`scripts/eval_local.py`):
- Runs evaluations locally without LangSmith
- Loads ground truth from local JSON files
- Creates mock Run/Example objects for evaluators
- Saves results to `evaluation_results/eval_{timestamp}.json`
- No API key or network connection required

### 4. Quality Tier System

**Mock** (Current):
- Generated by current model (gpt-4o-mini)
- Marked as "NEEDS_REFINEMENT"
- Used as placeholder for testing

**Silver** (Future):
- Generated by expensive model (gpt-4o, claude-opus)
- Better quality, may need light review

**Gold** (Manual):
- Manually refined by human
- Perfect, validated ground truth
- Used for production evaluation

## End-to-End Workflows

### Workflow A: Local Evaluation (No LangSmith Required)

```bash
# 1. Generate ground truth
dr --doppler eval generate-ground-truth --num 5

# 2. Check status
dr eval status

# 3. Run local evaluation
dr --doppler eval agent --dataset ground_truth/ --local

# 4. View results
cat evaluation_results/eval_*.json | jq '.summary.avg_scores'
```

### Workflow B: Remote Evaluation (LangSmith)

#### Phase 1: Generate Mock Ground Truth
```bash
# Option 1: Use doppler (recommended)
dr --doppler eval generate-ground-truth --num 5

# Option 2: Set environment variables manually
export OPENAI_API_KEY="..."
export LANGSMITH_API_KEY="..."
dr eval generate-ground-truth --num 5

# Files created in ground_truth/ground_truth_*.json
```

### Phase 2: Upload to LangSmith
```bash
# Upload agent-level dataset
dr --doppler eval upload-dataset --from ground_truth/ --name dr-eval-mock-v1 --type agent

# Upload component-level dataset
dr --doppler eval upload-dataset --from ground_truth/ --name dr-eval-mock-v1 --type component
```

### Phase 3: Run Evaluations
```bash
# Run agent-level evaluation
dr --doppler eval agent --dataset dr-eval-mock-v1

# Run component-level evaluation
dr --doppler eval component report-generation --dataset dr-eval-mock-v1
```

### Phase 4: Manual Refinement (Later)
```bash
# 1. Review and refine ground_truth/*.json files manually
# 2. Fix hallucinations, improve completeness, validate numbers
# 3. Change quality_tier to "gold" and status to "REFINED"

# 4. Re-upload refined dataset
dr eval upload-dataset --from ground_truth/ --name dr-eval-gold-v1 --type agent

# 5. Re-run evaluations with real ground truth
dr eval agent --dataset dr-eval-gold-v1
```

## File Format

### Ground Truth JSON
```json
{
  "ticker": "PTT",
  "date": "2025-11-20",
  "data": {
    "indicators": {...},
    "prices": {...},
    "news": [...],
    "percentiles": {...}
  },
  "ground_truth_report": "Thai report text...",
  "metadata": {
    "quality_tier": "mock",
    "needs_refinement": true,
    "generated_by": "gpt-4o-mini",
    "generated_at": "2025-11-22T10:30:00Z",
    "status": "DRAFT",
    "scores": {
      "faithfulness": 0.82,
      ...
    }
  }
}
```

## Evaluators Used (All 7)

1. **Faithfulness**: Numeric accuracy vs ground truth
2. **Completeness**: Coverage of analytical dimensions
3. **Reasoning Quality**: Quality of explanations
4. **Compliance**: Format/policy adherence
5. **Hallucination (LLM)**: LLM-as-judge semantic validation
6. **QoS**: System performance metrics
7. **Cost**: Operational costs

## Notes

- **Mock data limitation**: Comparing LLM to itself yields high scores (~0.95-1.0)
- **Manual refinement**: Required for accurate evaluation metrics
- **Historical data as truth anchor**: Use actual past data for validation
- **Two-tier evaluation**: Same metrics for both agent and component levels
- **Impact tracking**: Compare component vs agent scores to measure propagation

## Next Steps

1. Generate initial mock ground truth (5 examples)
2. Test evaluation infrastructure
3. Manually refine 5 examples to gold standard
4. Re-run evaluations with real ground truth
5. Expand dataset to 20-50 examples over time
6. Consider expensive model generation (silver tier) if manual refinement too time-consuming

## Environment Variables Required

```bash
export OPENAI_API_KEY="sk-..."           # For report generation
export LANGSMITH_API_KEY="lsv2_pt_..."   # For LangSmith uploads/evaluation
```

Or use the --doppler flag (recommended):
```bash
# The --doppler flag automatically wraps commands with doppler
# It re-executes: doppler run --project rag-chatbot-worktree --config dev_personal -- <command>

dr --doppler eval generate-ground-truth --num 5
dr --doppler eval upload-dataset --from ground_truth/ --name test-v1 --type agent
dr --doppler eval agent --dataset test-v1

# For local evaluation (no LangSmith needed)
dr --doppler eval agent --dataset ground_truth/ --local  # ✅ Works!
```

**How --doppler works:**
1. Detects the `--doppler` flag
2. Re-executes the command via `doppler run --project rag-chatbot-worktree --config dev_personal`
3. Doppler injects all environment variables from the cloud
4. Command runs with full env vars (OPENAI_API_KEY, LANGSMITH_API_KEY, etc.)

**Local evaluation features:**
- `--local` flag disables LangSmith tracing (sets `LANGSMITH_TRACING_V2=false`)
- No API calls to LangSmith servers
- Results saved to `evaluation_results/eval_{timestamp}.json`
- No network connection required (except for LLM calls)

## Tests

Comprehensive test suite for all eval commands:

```bash
# Run all eval CLI tests
python -m pytest tests/test_cli/test_eval_commands.py -v
```

**Test Coverage** (14 tests, all passing ✅):
- ✅ CLI help messages for all commands
- ✅ Generate ground truth with mock data
- ✅ Generate ground truth with no historical data
- ✅ Upload dataset to LangSmith
- ✅ Upload with no files
- ✅ Eval agent command
- ✅ Eval component command
- ✅ Invalid component name handling
- ✅ Quality tier support (mock/silver/gold)
- ✅ Agent vs component dataset type differences
