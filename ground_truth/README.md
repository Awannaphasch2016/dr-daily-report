# Ground Truth Dataset Directory

> **Note**: LangSmith has been deprecated. This directory structure is preserved for future evaluation frameworks (Langfuse planned).

This directory stores ground truth examples for offline evaluation.

## Structure

Each ground truth example is stored as a JSON file with the following naming convention:
```
ground_truth_<ticker>_<date>.json
```

Example: `ground_truth_PTT_2025-11-20.json`

## File Format

```json
{
  "ticker": "PTT",
  "date": "2025-11-20",
  "data": {
    "indicators": {
      "rsi": 55.71,
      "macd": 0.46,
      ...
    },
    "prices": {
      "open": 53.50,
      "close": 53.67,
      ...
    },
    "news": [
      {
        "title": "...",
        "sentiment": "positive",
        ...
      }
    ],
    "percentiles": {
      "rsi": {
        "percentile": 12.96,
        ...
      }
    }
  },
  "ground_truth_report": "ภาพรวม... (Thai report text)",
  "metadata": {
    "quality_tier": "mock|silver|gold",
    "needs_refinement": true|false,
    "generated_by": "gpt-4o-mini",
    "generated_at": "2025-11-22T10:30:00Z",
    "status": "DRAFT|REFINED",
    "scores": {
      "faithfulness": 0.82,
      "completeness": 0.75,
      "reasoning_quality": 0.78,
      "compliance": 0.88,
      "qos": 0.85,
      "cost": 0.90
    }
  }
}
```

## Quality Tiers

- **mock**: Generated by current model (cheap, needs manual refinement)
- **silver**: Generated by expensive model (good quality, light review needed)
- **gold**: Manually refined by human (perfect, validated)

## Workflow

### Workflow A: Local Evaluation (No LangSmith Required)

#### 1. Generate Mock Ground Truth

```bash
dr eval generate-ground-truth --num 5
```

This creates 5 JSON files in this directory with quality_tier="mock" and status="DRAFT".

#### 2. Run Local Evaluation

```bash
# Agent-level evaluation (end-to-end)
dr eval agent --dataset ground_truth/ --local

# Component-level evaluation (isolated LLM call)
dr eval component report-generation --dataset ground_truth/ --local
```

Results are saved to `evaluation_results/eval_{timestamp}.json`.

#### 3. View Results

```bash
# View latest results
cat ../evaluation_results/eval_*.json | jq '.summary.avg_scores'

# List all datasets and results
dr eval list-datasets --local

# Check system status
dr eval status
```

### Workflow B: Remote Evaluation (LangSmith)

#### 1. Generate Mock Ground Truth

```bash
dr eval generate-ground-truth --num 5
```

This creates 5 JSON files in this directory with quality_tier="mock" and status="DRAFT".

#### 2. Manual Refinement (Optional)

Review each JSON file and:
- Validate all numbers against the `data` field
- Fix hallucinations (incorrect claims)
- Improve completeness (add missing analysis)
- Perfect reasoning (enhance explanations)
- Update metadata:
  - Change `status` to "REFINED"
  - Change `quality_tier` to "gold"
  - Set `needs_refinement` to false

#### 3. Upload to LangSmith

```bash
# Agent-level dataset
dr eval upload-dataset --from ground_truth/ --name dr-eval-v1 --type agent

# Component-level dataset
dr eval upload-dataset --from ground_truth/ --name dr-eval-v1 --type component
```

#### 4. Run Evaluation

```bash
# Agent-level evaluation
dr eval agent --dataset dr-eval-v1

# Component-level evaluation
dr eval component report-generation --dataset dr-eval-v1
```

## Notes

- Keep original data intact in the `data` field - this serves as the "truth anchor" for validation
- Mock examples will result in high evaluation scores (~0.95-1.0) because you're comparing LLM output to itself
- After manual refinement to gold tier, scores will reflect actual model quality vs perfect ground truth
- Version your datasets (e.g., dr-eval-v1, dr-eval-v2) when making significant changes
