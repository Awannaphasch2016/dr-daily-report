# S3 Deployment & Usage Guide

**Last Updated**: 2025-11-11
**Status**: âœ… Deployed and Operational

## Overview

This project uses AWS S3 for three distinct purposes:
1. **Report Caching** - Hybrid SQLite + S3 caching for cross-Lambda persistence
2. **Dependency Loading** - Loading heavy data science libraries from S3 to bypass Lambda limits
3. **Data Lake** (future) - Long-term storage of raw and processed data for reprocessing

---

## 1. S3 Report Caching

### Problem Statement

Lambda functions store cache in SQLite within `/tmp` directory, which is:
- **Ephemeral** - Cleared on cold start (~15 minutes of inactivity)
- **Instance-specific** - Not shared across multiple Lambda instances
- **Inefficient** - Every new Lambda instance regenerates all reports

**Impact:**
- Users experienced 30+ second wait times for previously requested tickers
- Redundant OpenAI API calls ($0.03 per request) for same ticker/date
- High operational costs due to repeated expensive LLM calls

### Solution: Hybrid Caching Strategy

Two-tier caching system combining fast local cache with persistent cross-instance cache:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Lambda Request                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Layer 1: SQLite     â”‚ â† Fast (1ms)
          â”‚ Local /tmp cache    â”‚ â† Warm Lambda only
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ MISS
                 â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Layer 2: S3 Cache   â”‚ â† Persistent (100ms)
          â”‚ Cross-instance      â”‚ â† All Lambdas
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ MISS
                 â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Generate Report     â”‚ â† Expensive (79s)
          â”‚ yfinance + LLM      â”‚ â† $0.03 cost
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Save to SQLite + S3        â”‚
    â”‚ (Parallel write-through)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### S3 Cache Structure

```
s3://line-bot-pdf-reports-{account_id}/
â”œâ”€â”€ cache/
â”‚   â””â”€â”€ reports/
â”‚       â””â”€â”€ {ticker}/
â”‚           â””â”€â”€ {date}/
â”‚               â”œâ”€â”€ report.json    # Complete report data
â”‚               â”œâ”€â”€ chart.b64      # Chart image, base64
â”‚               â””â”€â”€ news.json      # News data
â””â”€â”€ reports/
    â””â”€â”€ {ticker}/
        â””â”€â”€ {date}.pdf             # Generated PDF
```

### Implementation

#### S3 Cache Manager (`src/s3_cache.py`)

Central module for all S3 caching operations:

**Core Methods:**
```python
# Generic operations
get_json(cache_type, ticker, date, filename) -> Dict
put_json(cache_type, ticker, date, filename, data) -> bool
get_text(cache_type, ticker, date, filename) -> str
put_text(cache_type, ticker, date, filename, text) -> bool

# High-level methods
get_cached_report(ticker, date) -> Dict
save_report_cache(ticker, date, report_data) -> bool
get_cached_chart(ticker, date) -> str
save_chart_cache(ticker, date, chart_b64) -> bool
get_cached_news(ticker, date) -> list
save_news_cache(ticker, date, news_data) -> bool
get_pdf_url(ticker, date) -> str
```

**TTL Implementation:**
```python
def _get_expiration_metadata(self) -> Dict[str, str]:
    expires_at = datetime.now() + timedelta(hours=self.ttl_hours)
    return {'expires-at': expires_at.isoformat()}

def _is_expired(self, metadata: Dict[str, str]) -> bool:
    expires_at_str = metadata.get('expires-at')
    if not expires_at_str:
        return True
    expires_at = datetime.fromisoformat(expires_at_str)
    return datetime.now() > expires_at
```

#### Database Integration (`src/database.py`)

Hybrid cache retrieval:
```python
def get_cached_report(self, ticker, date):
    # Layer 1: Check local SQLite (1ms)
    local_result = self._check_sqlite(ticker, date)
    if local_result:
        return local_result

    # Layer 2: Check S3 cache (100ms)
    if self.s3_cache:
        s3_result = self.s3_cache.get_cached_report(ticker, date)
        if s3_result:
            self._backfill_local_cache(ticker, date, s3_result)
            return s3_result['report_text']

    return None  # Cache miss
```

Write-through caching:
```python
def save_report(self, ticker, date, report_data):
    # Save to SQLite (local)
    self._save_to_sqlite(ticker, date, report_data)

    # Save to S3 (persistent)
    if self.s3_cache:
        self.s3_cache.save_report_cache(ticker, date, report_data)
```

#### LINE Bot Integration (`src/line_bot.py`)

S3 cache initialization:
```python
def __init__(self):
    # Initialize S3 cache
    cache_backend = os.getenv("CACHE_BACKEND", "hybrid")
    if cache_backend in ("s3", "hybrid"):
        pdf_bucket = os.getenv("PDF_BUCKET_NAME")
        cache_ttl = int(os.getenv("CACHE_TTL_HOURS", "24"))
        self.s3_cache = S3Cache(bucket_name=pdf_bucket, ttl_hours=cache_ttl)

    # Pass S3 cache to database
    self.db = TickerDatabase(s3_cache=self.s3_cache)
```

PDF URL reuse:
```python
# Check if PDF already exists in S3
if self.s3_cache:
    pdf_url = self.s3_cache.get_pdf_url(matched_ticker, today)
    if pdf_url:
        logger.info("âœ… PDF cache hit, reusing existing PDF")

# Generate new PDF only if not cached
if not pdf_url:
    pdf_bytes = self.agent.generate_pdf_report(matched_ticker)
    pdf_url = self.pdf_storage.upload_and_get_url(pdf_bytes, matched_ticker)
```

### Performance Results

#### Response Time Comparison

| Scenario | Before S3 Cache | After S3 Cache | Improvement |
|----------|-----------------|----------------|-------------|
| **First request (today)** | 79s | 79s | Same (must generate) |
| **Second request (same instance)** | 79s | 0.001s | **79,000x faster** (SQLite) |
| **Second request (different instance)** | 79s | 0.8s | **95x faster** (S3) |
| **Next day (cache expired)** | 79s | 79s | Same (regenerates) |

#### Cost Savings

**OpenAI API Costs:**
```
Model: GPT-4o
Input tokens: ~10,000 (context + data)
Output tokens: ~2,000 (Thai report)
Cost per request: ~$0.03

Before S3 Cache (100 requests/day for popular ticker):
  100 requests Ã— $0.03 = $3.00/day = $90/month

After S3 Cache (100 requests/day for popular ticker):
  1 generation Ã— $0.03 = $0.03/day = $0.90/month

Savings: $89.10/month per ticker (99% reduction)
```

**S3 Storage Costs:**
```
Storage:
  Average report size: 7KB
  1000 unique ticker-date combinations: 7MB
  Cost: 7MB Ã— $0.023/GB/month = $0.00016/month

Requests:
  Cache checks: 10,000/day Ã— 30 days = 300,000 GET requests
  Cost: 300,000 Ã— $0.0004/1000 = $0.12/month

Total S3 Cost: ~$0.12/month
```

**Net Savings:**
```
For a system serving 10 popular tickers with 100 requests/day each:

Before: $90/month Ã— 10 tickers = $900/month
After: $0.90/month Ã— 10 tickers + $0.12 S3 = $9.12/month

Monthly Savings: $890.88 (99% reduction)
Annual Savings: $10,690.56
```

### Configuration

**Environment Variables:**
```bash
CACHE_BACKEND=hybrid                     # Options: hybrid, s3, sqlite
CACHE_TTL_HOURS=24                       # Cache expiration in hours
PDF_BUCKET_NAME=line-bot-pdf-reports-{account_id}
PDF_URL_EXPIRATION_HOURS=24              # Presigned URL expiration
```

**Terraform Configuration:**
```hcl
resource "aws_s3_bucket_lifecycle_configuration" "pdf_reports" {
  bucket = aws_s3_bucket.pdf_reports.id

  # Rule 1: Delete old PDFs
  rule {
    id     = "delete_old_pdfs"
    status = "Enabled"
    prefix = "reports/"
    expiration { days = 30 }
  }

  # Rule 2: Delete old cache
  rule {
    id     = "delete_old_cache"
    status = "Enabled"
    prefix = "cache/"
    expiration { days = 1 }  # 24-hour cache TTL
  }
}
```

### Monitoring

**CloudWatch Logs:**
```
[INFO] âœ… SQLite cache hit for DBS19 on 2025-11-11
[INFO] âœ… S3 cache hit for DBS19 on 2025-11-11 (backfilled to SQLite)
[INFO] ðŸ’¾ Saved report to S3 cache: DBS19 on 2025-11-11
```

**Performance Metrics:**
```
REPORT RequestId: 32a2db40-ec16-42da-9ed9-a134d881b8c8
Duration: 8023.05 ms  (warm Lambda with SQLite cache)
Memory Used: 312 MB

REPORT RequestId: 01357683-9969-49a2-93fd-f41ded267e05
Duration: 79000 ms  (cache miss, full generation)
Memory Used: 345 MB
```

---

## 2. S3 Dependency Loading

### Problem Statement

Lambda deployment packages have a **250MB ZIP limit**, but data science libraries (numpy, pandas, matplotlib) exceed this:
- numpy: ~50MB
- pandas: ~40MB
- matplotlib: ~30MB
- **Total**: ~75MB compressed, ~150MB uncompressed

### Solution: S3 + /tmp Hybrid Deployment

Load heavy dependencies from S3 at runtime, storing them in Lambda's ephemeral `/tmp` storage:

```
Lambda Cold Start Flow:
1. Lambda handler starts
2. dependency_loader.py checks /tmp/python-libs
3. If not exists â†’ Download from S3 â†’ Extract to /tmp â†’ Add to sys.path
4. Import heavy modules (numpy, pandas, matplotlib)
5. Continue normal execution

Warm Invocations:
- Libraries already in /tmp â†’ Skip download â†’ Fast startup
```

### Implementation

#### Dependency Loader (`src/dependency_loader.py`)

Handles S3 download and extraction of heavy dependencies:

```python
import os
import sys
import zipfile
import boto3
from pathlib import Path

S3_BUCKET = "line-bot-ticker-deploy-20251030"
S3_KEY = "python-libs/data-science-libs.zip"
TMP_DIR = "/tmp/python-libs"

def load_heavy_dependencies():
    """Load heavy dependencies from S3 if not already in /tmp"""
    if Path(TMP_DIR).exists():
        # Already loaded in this container
        sys.path.insert(0, TMP_DIR)
        return

    # Download from S3
    s3 = boto3.client('s3')
    zip_path = "/tmp/data-science-libs.zip"
    s3.download_file(S3_BUCKET, S3_KEY, zip_path)

    # Extract to /tmp
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(TMP_DIR)

    # Add to sys.path
    sys.path.insert(0, TMP_DIR)

    # Clean up zip
    os.remove(zip_path)
```

#### Lambda Handler Integration (`src/lambda_handler.py`)

Lazy loading at module level:
```python
# Load heavy dependencies from S3 if needed
from src.dependency_loader import load_heavy_dependencies
load_heavy_dependencies()

# Now safe to import heavy libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

### Performance Characteristics

**Cold Start (first invocation):**
- Download time: ~2-5 seconds (depends on network)
- Extraction time: ~1-2 seconds
- Total cold start penalty: ~3-7 seconds

**Warm Start (subsequent invocations):**
- No download needed
- Libraries reused from /tmp
- Normal execution speed

### Configuration

**S3 Bucket/Key:**
```python
S3_BUCKET = "line-bot-ticker-deploy-20251030"
S3_KEY = "python-libs/data-science-libs.zip"
```

**/tmp Directory:**
```python
TMP_DIR = "/tmp/python-libs"
```

**Ephemeral Storage:**
```hcl
# terraform/main.tf
ephemeral_storage {
  size = 512  # Default: 512MB, can increase up to 10GB
}
```

### Benefits and Trade-offs

**Benefits:**
- âœ… Bypasses 250MB ZIP limit
- âœ… No new AWS permissions needed (uses existing S3 access)
- âœ… Warm invocations unaffected (libraries cached in /tmp)
- âœ… Works with current IAM setup (no ECR permissions required)

**Trade-offs:**
- âš ï¸ Cold start penalty: +3-7 seconds on first invocation
- âš ï¸ /tmp space usage: ~75MB for extracted libraries
- âš ï¸ Network dependency: Requires S3 access (usually reliable)

### Monitoring

**CloudWatch Logs:**
```
[INFO] Downloading heavy dependencies from s3://... (cold start)
[INFO] Heavy dependencies already loaded from /tmp (warm start)
[INFO] Heavy dependencies loaded successfully
```

---

## 3. S3 Data Lake (Future)

### Phase 1: Raw Data Storage âœ… Complete

**Status**: âœ… Operational

Raw yfinance data staged to S3 with versioning, tagging, and lifecycle policies:

```
s3://bucket/
â””â”€â”€ raw/yfinance/{ticker}/{date}/{timestamp}.json
```

**Features:**
- Versioning enabled
- Tags: `source: yfinance`, `ticker: {ticker}`, `fetched_at: {date}`
- Lifecycle: 90d â†’ Glacier, 365d â†’ Deep Archive

### Phase 2: Processed Data Storage (Planned)

**Goal**: Store computed indicators and percentiles alongside raw data for complete data lineage and reprocessing capabilities.

**Structure:**
```
s3://bucket/
â”œâ”€â”€ raw/yfinance/{ticker}/{date}/{timestamp}.json          # Phase 1 âœ…
â””â”€â”€ processed/
    â”œâ”€â”€ indicators/{ticker}/{date}/{timestamp}.json        # Phase 2
    â”œâ”€â”€ percentiles/{ticker}/{date}/{timestamp}.json       # Phase 2
    â””â”€â”€ features/{ticker}/{date}/{timestamp}.json          # Phase 2 (future)
```

**Key Features:**
- **Versioning**: Each computation creates new version (immutable)
- **Tagging**: Link processed data to source raw data version
- **Metadata**: Store computation metadata (timestamp, version, source S3 key)
- **Lifecycle**: Same lifecycle policies as raw data (90d â†’ Glacier, 365d â†’ Deep Archive)

### Data Lineage Tracking

**Tag Structure:**
```json
{
  "source": "computed",
  "ticker": "NVDA",
  "computed_at": "2025-01-15",
  "source_raw_data": "raw/yfinance/NVDA/2025-01-15/20250115_120000.json",
  "computation_version": "1.0",
  "computation_type": "indicators"
}
```

**Metadata Structure:**
```json
{
  "computed_at": "2025-01-15T12:30:00Z",
  "source": "indicators_computation",
  "ticker": "NVDA",
  "source_raw_data_key": "raw/yfinance/NVDA/2025-01-15/20250115_120000.json",
  "computation_version": "1.0",
  "data_classification": "computed-data",
  "indicators_computed": ["sma_20", "sma_50", "rsi_14", "macd", "bb_upper", "bb_lower"]
}
```

### Implementation Plan

**Step 1**: Extend DataLakeStorage Module (`src/data/data_lake.py`)
- Add `store_indicators()` method
- Add `store_percentiles()` method
- Add `store_features()` method (future)

**Step 2**: Update Indicator Computation
- After computing indicators â†’ Store to Aurora (existing)
- After computing indicators â†’ Store to S3 Data Lake (new)

**Step 3**: Add Retrieval Methods
- `get_latest_indicators()` - Get most recent indicators
- `get_indicators_by_date()` - Get indicators for specific date
- `get_indicators_by_version()` - Get specific version for reprocessing

**Step 4**: Update Terraform (if needed)
- Lifecycle policies for `processed/` prefix already exist

### Future Phases

**Phase 3**: Query/Retrieval API
- Athena integration for SQL queries over S3 data
- Glue catalog for schema discovery
- API endpoints to query data lake

**Phase 4**: Data Quality Validation
- Schema validation before storage
- Data quality checks (completeness, accuracy)
- Automated alerts on data quality issues

---

## IAM Permissions

### Required S3 Permissions

```json
{
  "Effect": "Allow",
  "Action": ["s3:GetObject"],
  "Resource": "arn:aws:s3:::line-bot-pdf-reports-{account_id}/python-libs/*"
},
{
  "Effect": "Allow",
  "Action": ["s3:GetObject", "s3:PutObject"],
  "Resource": "arn:aws:s3:::line-bot-pdf-reports-{account_id}/cache/*"
},
{
  "Effect": "Allow",
  "Action": ["s3:GetObject", "s3:PutObject"],
  "Resource": "arn:aws:s3:::line-bot-pdf-reports-{account_id}/reports/*"
}
```

**Status**: âœ… All permissions already granted (no new IAM changes required)

---

## Security Considerations

### Data Privacy
- Cache contains only technical analysis (no PII)
- S3 bucket has public access blocked
- Presigned URLs expire after 24 hours
- Cache objects auto-delete after 24 hours (lifecycle rule)

### Encryption
- S3 bucket encryption enabled (AES-256)
- Data encrypted at rest
- Data encrypted in transit (HTTPS)

---

## Troubleshooting

### S3 Cache Issues

**Problem**: Cache misses despite previous requests

**Diagnosis:**
```bash
# Check if cache exists
aws s3 ls s3://line-bot-pdf-reports-{account_id}/cache/reports/{ticker}/{date}/

# Check object metadata
aws s3api head-object \
  --bucket line-bot-pdf-reports-{account_id} \
  --key cache/reports/{ticker}/{date}/report.json
```

**Common Causes:**
- TTL expired (check `expires-at` metadata)
- Cache not saved (check CloudWatch logs for save confirmation)
- Different Lambda instance (S3 cache should handle this)

### Dependency Loading Issues

**Problem**: ImportError for numpy/pandas/matplotlib

**Diagnosis:**
```bash
# Check CloudWatch logs for dependency_loader errors
aws logs tail /aws/lambda/line-bot-ticker-report --follow
```

**Common Causes:**
- S3 bucket permissions incorrect
- S3 object doesn't exist at specified path
- /tmp insufficient space (increase ephemeral storage)

**Solution:**
```hcl
# terraform/main.tf - Increase ephemeral storage
ephemeral_storage {
  size = 10240  # 10GB in MB
}
```

### Slow Cold Starts

**Expected Behavior:**
- First invocation: 3-7 seconds overhead (S3 download + extraction)
- Subsequent invocations: No overhead (libraries cached in /tmp)

**Mitigation:**
- Increase Lambda timeout if processing + cold start > current timeout
- Consider pre-warming Lambda via scheduled events
- Monitor cold start frequency (if too frequent, may need provisioned concurrency)

---

## Best Practices

### Cache Management

1. **Monitor Cache Hit Ratio**
   - Track via CloudWatch logs
   - Aim for >80% cache hit rate for popular tickers

2. **Tune TTL Based on Usage**
   - High-volume tickers: Shorter TTL (6 hours) for fresher data
   - Low-volume tickers: Longer TTL (72 hours) to reduce costs
   - Default: 24 hours

3. **Pre-Warm Cache for Popular Tickers**
   ```python
   # Scheduled Lambda (CloudWatch Events)
   # Runs daily at market open
   def pre_warm_cache(event, context):
       popular_tickers = ["DBS19", "THAIBEV19", "PTT19"]
       for ticker in popular_tickers:
           if not cache_exists(ticker, today):
               generate_and_cache_report(ticker)
   ```

### Dependency Management

1. **Keep Heavy Dependencies Separate**
   - Minimal requirements in deployment package
   - Heavy dependencies in S3
   - Clear separation reduces deployment package size

2. **Version Control Dependencies**
   - Tag S3 objects with version
   - Enables rollback if needed

3. **Monitor /tmp Usage**
   - Track via CloudWatch metrics
   - Alert if approaching limit

### Data Lake Management

1. **Tag All Objects**
   - Consistent tagging enables data lineage
   - Tags enable filtering and lifecycle policies

2. **Use Lifecycle Policies**
   - Transition to cheaper storage tiers over time
   - Automatic deletion after retention period

3. **Validate Before Storage**
   - Schema validation prevents corrupt data
   - Data quality checks catch issues early

---

## References

- **Implementation Report**: S3 Cache Implementation Report (2025-11-11)
- **Terraform Module**: `terraform/modules/s3-data-lake/main.tf`
- **Code Files**:
  - `src/s3_cache.py` (430 lines)
  - `src/dependency_loader.py`
  - `src/data/data_lake.py`
  - `src/database.py` (S3 integration)
  - `src/line_bot.py` (S3 cache init)

---

## Changelog

### 2025-11-11
- âœ… S3 cache fully deployed and operational
- âœ… 95x performance improvement on cache hits
- âœ… 90% cost reduction for repeated ticker requests
- âœ… S3 dependency loading operational

### Future
- [ ] Implement Phase 2: Processed data storage
- [ ] Add cache analytics dashboard
- [ ] Implement intelligent TTL based on ticker activity
- [ ] Add Athena query support for data lake
