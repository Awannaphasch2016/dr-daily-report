"""
Reasoning Quality Scorer for Narrative Reports

Measures the quality of explanations and reasoning in LLM-generated narratives.
Evaluates clarity, coverage, specificity, alignment, minimality, and consistency.
"""

import re
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass


@dataclass
class ReasoningQualityScore:
    """Container for reasoning quality scoring results"""
    overall_score: float  # 0-100
    dimension_scores: Dict[str, float]  # Individual dimension scores
    issues: List[str]  # List of reasoning quality issues
    strengths: List[str]  # List of reasoning quality strengths


class ReasoningQualityScorer:
    """
    Score reasoning quality of narrative explanations
    
    Checks:
    1. Clarity - Are explanations clear and easy to understand?
    2. Coverage - Does reasoning cover all relevant aspects?
    3. Specificity - Are explanations specific rather than generic?
    4. Alignment - Do explanations align with data/claims?
    5. Minimality - Is reasoning concise without being incomplete?
    6. Consistency - Are explanations internally consistent?
    """
    
    def __init__(self):
        """Initialize reasoning quality scorer"""
        pass
    
    def score_narrative(
        self,
        narrative: str,
        indicators: Dict,
        percentiles: Dict,
        ticker_data: Dict
    ) -> ReasoningQualityScore:
        """
        Score reasoning quality of narrative
        
        Args:
            narrative: Generated Thai narrative text
            indicators: Technical indicators
            percentiles: Percentile data for historical context
            ticker_data: Ticker data including company info
        
        Returns:
            ReasoningQualityScore object with detailed scoring
        """
        issues = []
        strengths = []
        
        # 1. Clarity (20%)
        clarity_score, clarity_issues, clarity_strengths = self._check_clarity(
            narrative
        )
        issues.extend(clarity_issues)
        strengths.extend(clarity_strengths)
        
        # 2. Coverage (20%)
        coverage_score, coverage_issues, coverage_strengths = self._check_coverage(
            narrative, indicators, percentiles, ticker_data
        )
        issues.extend(coverage_issues)
        strengths.extend(coverage_strengths)
        
        # 3. Specificity (20%)
        specificity_score, specificity_issues, specificity_strengths = self._check_specificity(
            narrative
        )
        issues.extend(specificity_issues)
        strengths.extend(specificity_strengths)
        
        # 4. Alignment (15%)
        alignment_score, alignment_issues, alignment_strengths = self._check_alignment(
            narrative, indicators, percentiles
        )
        issues.extend(alignment_issues)
        strengths.extend(alignment_strengths)
        
        # 5. Minimality (15%)
        minimality_score, minimality_issues, minimality_strengths = self._check_minimality(
            narrative
        )
        issues.extend(minimality_issues)
        strengths.extend(minimality_strengths)
        
        # 6. Consistency (10%)
        consistency_score, consistency_issues, consistency_strengths = self._check_consistency(
            narrative
        )
        issues.extend(consistency_issues)
        strengths.extend(consistency_strengths)
        
        # Calculate overall score (weighted average)
        overall_score = (
            clarity_score * 0.20 +
            coverage_score * 0.20 +
            specificity_score * 0.20 +
            alignment_score * 0.15 +
            minimality_score * 0.15 +
            consistency_score * 0.10
        )
        
        dimension_scores = {
            'clarity': clarity_score,
            'coverage': coverage_score,
            'specificity': specificity_score,
            'alignment': alignment_score,
            'minimality': minimality_score,
            'consistency': consistency_score
        }
        
        return ReasoningQualityScore(
            overall_score=overall_score,
            dimension_scores=dimension_scores,
            issues=issues,
            strengths=strengths
        )
    
    def _check_clarity(self, narrative: str) -> Tuple[float, List[str], List[str]]:
        """Check if explanations are clear and easy to understand"""
        issues = []
        strengths = []
        
        narrative_lower = narrative.lower()
        
        # Check for vague terms that reduce clarity
        vague_terms = [
            'maybe', 'perhaps', 'might', 'could', 'possibly', 'might be',
            '‡∏≠‡∏≤‡∏à‡∏à‡∏∞', '‡∏≠‡∏≤‡∏à', '‡∏ö‡∏≤‡∏á‡∏ó‡∏µ', '‡∏ô‡πà‡∏≤‡∏à‡∏∞', '‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ'
        ]
        
        vague_count = sum(1 for term in vague_terms if term in narrative_lower)
        
        # Check for clear cause-effect relationships
        cause_effect_markers = [
            'because', '‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å', '‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤', '‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•', '‡πÄ‡∏û‡∏£‡∏≤‡∏∞',
            'therefore', '‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô', '‡∏à‡∏∂‡∏á', '‡∏ó‡∏≥‡πÉ‡∏´‡πâ', '‡∏™‡πà‡∏á‡∏ú‡∏•‡πÉ‡∏´‡πâ',
            'due to', '‡∏à‡∏≤‡∏Å', '‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏•‡∏°‡∏≤‡∏à‡∏≤‡∏Å'
        ]
        
        has_cause_effect = any(marker in narrative_lower for marker in cause_effect_markers)
        
        # Check for clear structure (sections, formatting)
        has_structure = any(marker in narrative for marker in ['üìñ', 'üí°', 'üéØ', '‚ö†Ô∏è', '**'])
        
        # Check for explanations (not just statements)
        explanation_markers = [
            '‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤', '‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á', '‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤', '‡∏ö‡πà‡∏á‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤',
            'means', 'indicates', 'shows', 'suggests', 'implies'
        ]
        
        has_explanations = any(marker in narrative_lower for marker in explanation_markers)
        
        # Check sentence length (very long sentences reduce clarity)
        sentences = re.split(r'[.!?„ÄÇ]', narrative)
        avg_sentence_length = sum(len(s.split()) for s in sentences if s.strip()) / max(len([s for s in sentences if s.strip()]), 1)
        
        # Scoring
        score = 100.0
        
        # Penalize excessive vague terms
        if vague_count > 5:
            score -= 20
            issues.append("‚ùå Too many vague terms reduce clarity")
        elif vague_count == 0:
            strengths.append("‚úÖ No vague terms - explanations are confident")
        
        # Reward cause-effect relationships
        if has_cause_effect:
            strengths.append("‚úÖ Clear cause-effect relationships present")
        else:
            score -= 15
            issues.append("‚ùå Missing clear cause-effect relationships")
        
        # Reward structured format
        if has_structure:
            strengths.append("‚úÖ Well-structured narrative format")
        else:
            score -= 10
            issues.append("‚ùå Narrative lacks clear structure")
        
        # Reward explanations
        if has_explanations:
            strengths.append("‚úÖ Explanations provided (not just statements)")
        else:
            score -= 15
            issues.append("‚ùå Missing explanations (only statements)")
        
        # Penalize overly long sentences
        if avg_sentence_length > 30:
            score -= 10
            issues.append("‚ùå Sentences too long (reduce clarity)")
        elif avg_sentence_length < 10:
            score -= 5
            issues.append("‚ö†Ô∏è Sentences too short (may lack detail)")
        
        return max(0, min(100, score)), issues, strengths
    
    def _check_coverage(self, narrative: str, indicators: Dict, percentiles: Dict, ticker_data: Dict) -> Tuple[float, List[str], List[str]]:
        """Check if reasoning covers all relevant aspects"""
        issues = []
        strengths = []
        
        narrative_lower = narrative.lower()
        
        # Check if multiple analytical dimensions are explained
        explained_dimensions = []
        
        # Technical indicators
        if any(keyword in narrative_lower for keyword in ['rsi', 'macd', 'sma', 'technical', '‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ']):
            explained_dimensions.append('technical')
        
        # Volatility/risk
        if any(keyword in narrative_lower for keyword in ['uncertainty', '‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô', 'atr', 'volatility', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô']):
            explained_dimensions.append('volatility')
        
        # Market sentiment
        if any(keyword in narrative_lower for keyword in ['vwap', 'buying pressure', 'selling pressure', '‡πÅ‡∏£‡∏á‡∏ã‡∏∑‡πâ‡∏≠', '‡πÅ‡∏£‡∏á‡∏Ç‡∏≤‡∏¢', 'sentiment']):
            explained_dimensions.append('sentiment')
        
        # Volume
        if any(keyword in narrative_lower for keyword in ['volume', '‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì', 'trading activity']):
            explained_dimensions.append('volume')
        
        # Historical context
        if any(keyword in narrative_lower for keyword in ['percentile', '‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡πÑ‡∏ó‡∏•‡πå', 'historical', '‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå', '‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï']):
            explained_dimensions.append('historical')
        
        # Fundamental (if available)
        if ticker_data.get('pe_ratio') or ticker_data.get('eps'):
            if any(keyword in narrative_lower for keyword in ['p/e', 'pe ratio', 'eps', 'fundamental', '‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô']):
                explained_dimensions.append('fundamental')
        
        # Scoring based on how many dimensions are explained
        total_dimensions = 5 if not (ticker_data.get('pe_ratio') or ticker_data.get('eps')) else 6
        coverage_ratio = len(explained_dimensions) / total_dimensions
        
        score = coverage_ratio * 100
        
        if coverage_ratio >= 0.8:
            strengths.append(f"‚úÖ Covers {len(explained_dimensions)}/{total_dimensions} analytical dimensions")
        elif coverage_ratio >= 0.6:
            issues.append(f"‚ö†Ô∏è Only covers {len(explained_dimensions)}/{total_dimensions} analytical dimensions")
        else:
            issues.append(f"‚ùå Covers only {len(explained_dimensions)}/{total_dimensions} analytical dimensions")
        
        # Check if reasoning explains WHY, not just WHAT
        why_markers = [
            '‡πÄ‡∏û‡∏£‡∏≤‡∏∞', '‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å', '‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤', '‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•', '‡∏ó‡∏≥‡πÑ‡∏°',
            'why', 'because', 'reason', 'rationale', 'cause'
        ]
        
        has_why = any(marker in narrative_lower for marker in why_markers)
        
        if has_why:
            strengths.append("‚úÖ Explains WHY (not just WHAT)")
        else:
            score -= 20
            issues.append("‚ùå Missing WHY explanations (only describes WHAT)")
        
        return max(0, min(100, score)), issues, strengths
    
    def _check_specificity(self, narrative: str) -> Tuple[float, List[str], List[str]]:
        """Check if explanations are specific rather than generic"""
        issues = []
        strengths = []
        
        narrative_lower = narrative.lower()
        
        # Check for generic phrases
        generic_phrases = [
            'generally', 'usually', 'typically', 'often', 'in general',
            '‡πÇ‡∏î‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ', '‡∏°‡∏±‡∏Å‡∏à‡∏∞', '‡∏õ‡∏Å‡∏ï‡∏¥', '‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ', '‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà'
        ]
        
        generic_count = sum(1 for phrase in generic_phrases if phrase in narrative_lower)
        
        # Check for specific numbers/data references
        number_patterns = [
            r'\d+\.?\d*%',  # Percentages
            r'\$\d+\.?\d*',  # Dollar amounts
            r'\d+\.?\d*x',  # Multipliers
            r'\d+/\d+',  # Ratios like 51/100
            r'‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡πÑ‡∏ó‡∏•‡πå\s*\d+\.?\d*%',  # Percentile mentions
        ]
        
        specific_numbers = sum(len(re.findall(pattern, narrative)) for pattern in number_patterns)
        
        # Check for specific comparisons
        comparison_markers = [
            'compared to', '‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö', '‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö', 'versus', 'vs',
            'higher than', 'lower than', '‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤', '‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤',
            'above', 'below', '‡πÄ‡∏´‡∏ô‡∏∑‡∏≠', '‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤'
        ]
        
        has_comparisons = any(marker in narrative_lower for marker in comparison_markers)
        
        # Check for named entities or specific references
        has_specific_refs = bool(re.search(r'DBS\d+|AAPL\d+|NVDA\d+', narrative)) or \
                           bool(re.search(r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó\s+\w+', narrative))
        
        # Scoring
        score = 100.0
        
        # Penalize excessive generic language
        if generic_count > 3:
            score -= 25
            issues.append("‚ùå Too many generic phrases (reduce specificity)")
        elif generic_count == 0:
            strengths.append("‚úÖ No generic phrases - explanations are specific")
        
        # Reward specific numbers
        if specific_numbers >= 5:
            strengths.append(f"‚úÖ Includes {specific_numbers} specific numbers/data points")
        elif specific_numbers >= 3:
            score -= 10
            issues.append("‚ö†Ô∏è Could include more specific numbers")
        else:
            score -= 30
            issues.append("‚ùå Lacks specific numbers/data points")
        
        # Reward comparisons
        if has_comparisons:
            strengths.append("‚úÖ Includes specific comparisons")
        else:
            score -= 15
            issues.append("‚ùå Missing specific comparisons")
        
        # Reward specific references
        if has_specific_refs:
            strengths.append("‚úÖ References specific entities/tickers")
        
        return max(0, min(100, score)), issues, strengths
    
    def _check_alignment(self, narrative: str, indicators: Dict, percentiles: Dict) -> Tuple[float, List[str], List[str]]:
        """Check if explanations align with data/claims"""
        issues = []
        strengths = []
        
        narrative_lower = narrative.lower()
        
        # Check if percentile claims align with percentile data
        percentile_mentions = re.findall(r'‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡πÑ‡∏ó‡∏•‡πå\s*(\d+\.?\d*)%', narrative)
        percentile_aligned = 0
        percentile_total = 0
        
        if percentile_mentions:
            for claimed_pct in percentile_mentions:
                percentile_total += 1
                claimed_value = float(claimed_pct)
                # Check if it matches any actual percentile (within 5%)
                aligned = False
                for metric, metric_data in percentiles.items():
                    if isinstance(metric_data, dict):
                        actual_pct = metric_data.get('percentile', 0)
                    else:
                        actual_pct = metric_data
                    
                    if abs(claimed_value - actual_pct) <= 5:
                        aligned = True
                        break
                
                if aligned:
                    percentile_aligned += 1
        
        # Check if uncertainty interpretations align with thresholds
        uncertainty_score = indicators.get('uncertainty_score', 0)
        uncertainty_aligned = False
        
        if uncertainty_score < 25:
            expected_terms = ['‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£', 'stable', '‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á']
        elif uncertainty_score < 50:
            expected_terms = ['‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á', 'moderate', '‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á']
        elif uncertainty_score < 75:
            expected_terms = ['‡∏™‡∏π‡∏á', 'high', '‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô']
        else:
            expected_terms = ['‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á', 'extreme', '‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å']
        
        if any(term in narrative_lower for term in expected_terms):
            uncertainty_aligned = True
        
        # Check if VWAP interpretations align
        # Calculate VWAP percentage from indicators
        current_price = indicators.get('current_price', 0)
        vwap = indicators.get('vwap', 0)
        vwap_pct = ((current_price - vwap) / vwap * 100) if vwap and vwap > 0 else 0
        
        vwap_aligned = False
        
        if vwap_pct >= 15:
            expected_terms = ['‡πÅ‡∏£‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡πÅ‡∏£‡∏á‡∏°‡∏≤‡∏Å', 'strong buying']
        elif vwap_pct >= 5:
            expected_terms = ['‡πÅ‡∏£‡∏á‡∏ã‡∏∑‡πâ‡∏≠', 'buying pressure']
        elif vwap_pct <= -15:
            expected_terms = ['‡πÅ‡∏£‡∏á‡∏Ç‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡∏°‡∏≤‡∏Å', 'strong selling']
        elif vwap_pct <= -5:
            expected_terms = ['‡πÅ‡∏£‡∏á‡∏Ç‡∏≤‡∏¢', 'selling pressure']
        else:
            expected_terms = ['‡∏™‡∏°‡∏î‡∏∏‡∏•', 'balanced', 'neutral']
        
        if any(term in narrative_lower for term in expected_terms):
            vwap_aligned = True
        
        # Scoring
        score = 100.0
        
        # Percentile alignment
        if percentile_total > 0:
            alignment_ratio = percentile_aligned / percentile_total
            if alignment_ratio >= 0.8:
                strengths.append(f"‚úÖ {percentile_aligned}/{percentile_total} percentile claims aligned")
            else:
                score -= 30 * (1 - alignment_ratio)
                issues.append(f"‚ùå Only {percentile_aligned}/{percentile_total} percentile claims aligned")
        
        # Uncertainty alignment
        if uncertainty_aligned:
            strengths.append("‚úÖ Uncertainty interpretation aligns with data")
        else:
            score -= 25
            issues.append("‚ùå Uncertainty interpretation doesn't align with score")
        
        # VWAP alignment
        if vwap_aligned:
            strengths.append("‚úÖ VWAP interpretation aligns with data")
        else:
            score -= 25
            issues.append("‚ùå VWAP interpretation doesn't align with data")
        
        return max(0, min(100, score)), issues, strengths
    
    def _check_minimality(self, narrative: str) -> Tuple[float, List[str], List[str]]:
        """Check if reasoning is concise without being incomplete"""
        issues = []
        strengths = []
        
        narrative_lower = narrative.lower()
        
        # Calculate basic metrics
        word_count = len(narrative.split())
        char_count = len(narrative)
        
        # Check for repetition
        sentences = re.split(r'[.!?„ÄÇ]', narrative)
        unique_sentences = len(set(s.strip().lower() for s in sentences if s.strip()))
        repetition_ratio = unique_sentences / max(len([s for s in sentences if s.strip()]), 1)
        
        # Check for redundant phrases
        redundant_phrases = [
            'in other words', '‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏≠‡∏µ‡∏Å‡∏ô‡∏±‡∏¢‡∏´‡∏ô‡∏∂‡πà‡∏á', '‡∏≠‡∏µ‡∏Å‡∏ô‡∏±‡∏¢‡∏´‡∏ô‡∏∂‡πà‡∏á',
            'to put it simply', '‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏á‡πà‡∏≤‡∏¢‡πÜ', '‡∏á‡πà‡∏≤‡∏¢‡πÜ',
            'as mentioned before', '‡∏î‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß', '‡∏ó‡∏µ‡πà‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß'
        ]
        
        redundant_count = sum(1 for phrase in redundant_phrases if phrase.lower() in narrative.lower())
        
        # Check for filler words
        filler_words = [
            'actually', 'basically', 'essentially', 'literally',
            '‡∏à‡∏£‡∏¥‡∏á‡πÜ', '‡πÇ‡∏î‡∏¢‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô', '‡πÇ‡∏î‡∏¢‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á', '‡∏à‡∏£‡∏¥‡∏á‡πÜ‡πÅ‡∏•‡πâ‡∏ß'
        ]
        
        filler_count = sum(1 for word in filler_words if word.lower() in narrative_lower)
        
        # Scoring
        score = 100.0
        
        # Optimal length: 200-800 words
        if 200 <= word_count <= 800:
            strengths.append(f"‚úÖ Optimal length ({word_count} words)")
        elif word_count < 200:
            score -= 20
            issues.append(f"‚ö†Ô∏è Too short ({word_count} words) - may lack detail")
        elif word_count > 1200:
            score -= 30
            issues.append(f"‚ùå Too long ({word_count} words) - needs conciseness")
        elif word_count > 800:
            score -= 15
            issues.append(f"‚ö†Ô∏è Slightly long ({word_count} words)")
        
        # Repetition penalty
        if repetition_ratio < 0.7:
            score -= 20
            issues.append("‚ùå Too much repetition reduces conciseness")
        elif repetition_ratio >= 0.9:
            strengths.append("‚úÖ No significant repetition")
        
        # Redundant phrases penalty
        if redundant_count > 2:
            score -= 15
            issues.append(f"‚ùå {redundant_count} redundant phrases found")
        elif redundant_count == 0:
            strengths.append("‚úÖ No redundant phrases")
        
        # Filler words penalty
        if filler_count > 3:
            score -= 10
            issues.append(f"‚ö†Ô∏è {filler_count} filler words found")
        
        return max(0, min(100, score)), issues, strengths
    
    def _check_consistency(self, narrative: str) -> Tuple[float, List[str], List[str]]:
        """Check if explanations are internally consistent"""
        issues = []
        strengths = []
        
        narrative_lower = narrative.lower()
        
        # Check for contradictory statements
        contradictions = []
        
        # Check for conflicting recommendations
        has_buy = any(term in narrative_lower for term in ['buy', '‡∏ã‡∏∑‡πâ‡∏≠', '‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ã‡∏∑‡πâ‡∏≠'])
        has_sell = any(term in narrative_lower for term in ['sell', '‡∏Ç‡∏≤‡∏¢', '‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Ç‡∏≤‡∏¢'])
        has_hold = any(term in narrative_lower for term in ['hold', '‡∏ñ‡∏∑‡∏≠', '‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ñ‡∏∑‡∏≠'])
        
        # Count distinct recommendations
        recommendations = sum([has_buy, has_sell, has_hold])
        if recommendations > 1:
            contradictions.append("Multiple conflicting recommendations (BUY/SELL/HOLD)")
        
        # Check for conflicting risk assessments
        risk_terms_high = ['high risk', 'high risk', 'risky', 'dangerous', '‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á', '‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏°‡∏≤‡∏Å']
        risk_terms_low = ['low risk', 'safe', 'stable', '‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ï‡πà‡∏≥', '‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢']
        
        has_high_risk = any(term in narrative_lower for term in risk_terms_high)
        has_low_risk = any(term in narrative_lower for term in risk_terms_low)
        
        if has_high_risk and has_low_risk:
            # Check context - if they're in different sections, it's OK
            risk_sections = narrative.split('‚ö†Ô∏è')
            if len(risk_sections) > 1:
                # OK - risk section can mention both
                pass
            else:
                contradictions.append("Conflicting risk assessments (high and low)")
        
        # Check for number consistency
        # Extract all numbers and check if same metric has different values
        numbers = re.findall(r'\d+\.?\d*', narrative)
        # This is a simple check - more sophisticated would require semantic understanding
        
        # Check for consistent terminology
        # Check if same concept is referred to differently (e.g., "uncertainty" vs "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô")
        # This is OK - actually good for bilingual content
        
        # Scoring
        score = 100.0
        
        if contradictions:
            score -= len(contradictions) * 30
            for contradiction in contradictions:
                issues.append(f"‚ùå {contradiction}")
        else:
            strengths.append("‚úÖ No internal contradictions found")
        
        # Check for consistent recommendation
        if recommendations == 1:
            strengths.append("‚úÖ Single clear recommendation (no conflicts)")
        elif recommendations == 0:
            score -= 20
            issues.append("‚ùå No clear recommendation")
        else:
            score -= 25
            issues.append("‚ö†Ô∏è Multiple or conflicting recommendations")
        
        return max(0, min(100, score)), issues, strengths
    
    def format_score_report(self, score: ReasoningQualityScore) -> str:
        """Format reasoning quality score as human-readable report"""
        report_lines = [
            "=" * 80,
            "REASONING QUALITY SCORE REPORT",
            "=" * 80,
            "",
            f"üìä Overall Reasoning Quality Score: {score.overall_score:.1f}/100",
            "",
            "Dimension Breakdown:",
        ]
        
        for dimension, value in score.dimension_scores.items():
            emoji = "‚úÖ" if value >= 80 else ("‚ö†Ô∏è" if value >= 60 else "‚ùå")
            report_lines.append(f"  {emoji} {dimension}: {value:.1f}/100")
        
        if score.issues:
            report_lines.extend([
                "",
                "‚ùå Reasoning Quality Issues:",
            ])
            for issue in score.issues:
                report_lines.append(f"  {issue}")
        
        if score.strengths:
            report_lines.extend([
                "",
                "‚úÖ Reasoning Quality Strengths:",
            ])
            for strength in score.strengths:
                report_lines.append(f"  {strength}")
        
        report_lines.extend([
            "",
            "=" * 80
        ])
        
        return "\n".join(report_lines)
