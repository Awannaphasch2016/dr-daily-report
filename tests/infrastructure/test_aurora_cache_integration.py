# -*- coding: utf-8 -*-
"""
Infrastructure tests for Aurora report caching from API worker

TDD tests to verify that:
1. Worker can store generated reports to Aurora cache
2. API can retrieve cached reports for instant response
3. Second request for same ticker returns cached response

These tests drive the requirement that the report worker must
store reports to Aurora in addition to DynamoDB jobs table.

Markers:
    @pytest.mark.integration - Requires real AWS/Aurora access
    @pytest.mark.infrastructure - Infrastructure verification tests
"""

import json
import pytest
from datetime import date
from unittest.mock import MagicMock, patch


@pytest.mark.unit
class TestPrecomputeServiceStoreReportFromAPI:
    """Test suite for storing API-generated reports to Aurora cache"""

    def test_store_report_from_api_method_exists(self):
        """Test that store_report_from_api method exists on PrecomputeService

        This drives the requirement that PrecomputeService must have a public
        method for storing reports generated by the API worker.
        """
        from src.data.aurora.precompute_service import PrecomputeService

        # The method should exist
        assert hasattr(PrecomputeService, 'store_report_from_api'), (
            "PrecomputeService must have store_report_from_api() method "
            "for workers to store generated reports to Aurora cache"
        )

    def test_store_report_from_api_accepts_required_params(self):
        """Test that store_report_from_api accepts all required parameters

        Required parameters based on API report structure:
        - symbol: ticker symbol (e.g., 'DBS19')
        - report_text: narrative report in Thai
        - report_json: full API response dict
        - strategy: generation strategy used
        """
        from src.data.aurora.precompute_service import PrecomputeService
        import inspect

        # Get method signature
        sig = inspect.signature(PrecomputeService.store_report_from_api)
        params = list(sig.parameters.keys())

        # Should have these parameters (excluding 'self')
        required_params = ['symbol', 'report_text', 'report_json']

        for param in required_params:
            assert param in params, (
                f"store_report_from_api must accept '{param}' parameter"
            )


@pytest.mark.unit
class TestReportWorkerAuroraCacheIntegration:
    """Test suite for worker storing reports to Aurora cache"""

    def test_worker_stores_report_to_aurora_after_completion(self):
        """Test that worker stores report to Aurora cache after completing job

        This test verifies the integration between report_worker_handler
        and PrecomputeService for cache write-through.
        """
        # This test checks that the worker code includes Aurora cache storage
        import ast
        from pathlib import Path

        worker_path = Path(__file__).parent.parent.parent / 'src' / 'report_worker_handler.py'
        with open(worker_path, 'r') as f:
            worker_code = f.read()

        # Parse the AST to check for PrecomputeService or store_report_from_api
        tree = ast.parse(worker_code)

        # Check if PrecomputeService is imported or used
        has_precompute_import = False
        has_store_call = False

        for node in ast.walk(tree):
            # Check imports
            if isinstance(node, ast.ImportFrom):
                if 'precompute_service' in (node.module or ''):
                    has_precompute_import = True

            # Check for store_report_from_api call
            if isinstance(node, ast.Call):
                if isinstance(node.func, ast.Attribute):
                    if node.func.attr == 'store_report_from_api':
                        has_store_call = True

        assert has_precompute_import, (
            "Worker must import PrecomputeService to store reports to Aurora cache"
        )
        assert has_store_call, (
            "Worker must call store_report_from_api() to cache reports in Aurora"
        )


@pytest.mark.integration
class TestAuroraCacheRoundTrip:
    """Integration tests for Aurora cache round-trip

    These tests require real Aurora connection and verify end-to-end
    cache storage and retrieval.

    Run with: pytest -m integration tests/infrastructure/test_aurora_cache_integration.py
    Requires: doppler run -- pytest ... (for Aurora credentials)
    """

    @pytest.fixture
    def sample_report_response(self):
        """Sample report response matching API format"""
        return {
            'ticker': 'TEST19',
            'company_name': 'Test Company',
            'price': 100.0,
            'stance': 'neutral',
            'narrative_report': 'ทดสอบรายงาน - Test report in Thai',
            'summary_sections': {
                'key_takeaways': ['Point 1', 'Point 2'],
                'risks_to_watch': ['Risk 1']
            },
            'technical_metrics': [],
            'fundamentals': {},
            'generation_metadata': {
                'strategy': 'multi_stage_analysis',
                'cache_hit': False
            }
        }

    def test_store_and_retrieve_report(self, sample_report_response):
        """Test storing a report and retrieving it from cache

        Verifies the full round-trip:
        1. Store report via store_report_from_api()
        2. Retrieve report via get_cached_report()
        3. Verify report content matches
        """
        pytest.skip("Integration test - requires Aurora connection")
        # Implementation would:
        # 1. Create PrecomputeService with real Aurora connection
        # 2. Store sample report
        # 3. Retrieve and verify


@pytest.mark.integration
class TestAuroraSchemaValidation:
    """Integration tests that validate SQL queries match actual Aurora schema.

    These tests catch bugs like:
    - Referencing non-existent columns (e.g., 'date' instead of 'report_date')
    - Column type mismatches
    - Missing required columns

    Bug History:
    - 2025-12-02: _store_completed_report referenced non-existent columns:
      'date', 'pdf_s3_key', 'pdf_generated_at', 'report_generated_at'
      Actual schema has: 'report_date', 'computed_at', 'expires_at'

    Run with:
        doppler run -- pytest -m integration tests/infrastructure/test_aurora_cache_integration.py -v
    """

    @pytest.fixture
    def aurora_client(self):
        """Get Aurora client with real connection.

        Skips test if:
        - Aurora credentials not available
        - Cannot connect to Aurora (VPC/network issues)
        """
        import os
        if not os.environ.get('AURORA_HOST') and not os.environ.get('AURORA_SECRET_ARN'):
            pytest.skip("Aurora credentials not configured (run with doppler)")

        from src.data.aurora.client import AuroraClient
        client = AuroraClient()

        # Test connection - skip if unreachable (VPC issues)
        try:
            with client.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("SELECT 1")
        except Exception as e:
            pytest.skip(f"Cannot connect to Aurora (VPC/network): {e}")

        return client

    def test_precomputed_reports_schema_matches_code(self, aurora_client):
        """Verify precomputed_reports table schema matches what code expects.

        This test catches column name mismatches between code and database.
        """
        # Get actual schema from database
        with aurora_client.get_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute("""
                    SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE
                    FROM INFORMATION_SCHEMA.COLUMNS
                    WHERE TABLE_SCHEMA = DATABASE()
                    AND TABLE_NAME = 'precomputed_reports'
                    ORDER BY ORDINAL_POSITION
                """)
                columns = cursor.fetchall()

        actual_columns = {row['COLUMN_NAME'] for row in columns}

        # Columns that _store_completed_report tries to INSERT
        code_expects = {
            'ticker_id', 'symbol', 'report_date',
            'report_text', 'report_json', 'strategy',
            'generation_time_ms', 'mini_reports', 'chart_base64',
            'status', 'expires_at', 'computed_at'
        }

        # Find mismatches
        missing_in_db = code_expects - actual_columns
        assert not missing_in_db, (
            f"Code references columns that don't exist in database: {missing_in_db}\n"
            f"Actual columns: {sorted(actual_columns)}"
        )

    def test_store_completed_report_sql_executes_successfully(self, aurora_client):
        """Test that _store_completed_report SQL actually executes against Aurora.

        This is the definitive test - if the SQL is wrong, this fails.
        Uses a test ticker that gets cleaned up.
        """
        from src.data.aurora.precompute_service import PrecomputeService
        from datetime import date

        service = PrecomputeService()

        # Use a clearly test ticker symbol
        test_symbol = "_TEST_CACHE_VALIDATION_"
        test_ticker_id = 99999

        try:
            # Try to execute the actual SQL
            service._store_completed_report(
                ticker_id=test_ticker_id,
                symbol=test_symbol,
                data_date=date.today(),
                report_text="Test report for schema validation",
                report_json={"test": True, "purpose": "schema_validation"},
                strategy="test",
                generation_time_ms=100,
                mini_reports={},
                chart_base64="",
            )

            # Verify it was inserted
            with aurora_client.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(
                        "SELECT * FROM precomputed_reports WHERE symbol = %s",
                        (test_symbol,)
                    )
                    result = cursor.fetchone()

            assert result is not None, "Report should have been inserted"
            assert result['ticker_id'] == test_ticker_id
            assert result['symbol'] == test_symbol
            assert result['status'] == 'completed'

        finally:
            # Cleanup test data
            with aurora_client.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(
                        "DELETE FROM precomputed_reports WHERE symbol = %s",
                        (test_symbol,)
                    )
                conn.commit()

    def test_store_report_from_api_end_to_end(self, aurora_client):
        """Test full store_report_from_api flow with real Aurora.

        This tests the complete flow including TickerResolver.
        """
        from src.data.aurora.precompute_service import PrecomputeService

        service = PrecomputeService()

        # Use a known ticker from tickers.csv
        test_ticker = "DBS19"

        try:
            result = service.store_report_from_api(
                symbol=test_ticker,
                report_text="Integration test report - ทดสอบ",
                report_json={
                    "ticker": test_ticker,
                    "test": True,
                    "narrative_report": "Integration test"
                },
                strategy="integration_test",
                chart_base64="test_base64",
            )

            assert result is True, (
                "store_report_from_api should return True on success. "
                "If False, check: 1) TickerResolver found ticker, 2) SQL executed"
            )

            # Verify it's in the database
            # DBS19 maps to D05.SI in Yahoo format
            with aurora_client.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        SELECT * FROM precomputed_reports
                        WHERE symbol = 'D05.SI'
                        AND report_date = CURDATE()
                        AND strategy = 'integration_test'
                    """)
                    result = cursor.fetchone()

            assert result is not None, "Report should exist in Aurora cache"

        finally:
            # Cleanup
            with aurora_client.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        DELETE FROM precomputed_reports
                        WHERE strategy = 'integration_test'
                    """)
                conn.commit()

    def test_invalid_column_would_fail(self, aurora_client):
        """Meta-test: Verify that referencing invalid columns DOES fail.

        This confirms our test approach is valid - if code referenced
        a non-existent column, the SQL would fail.
        """
        with aurora_client.get_connection() as conn:
            with conn.cursor() as cursor:
                with pytest.raises(Exception) as exc_info:
                    cursor.execute("""
                        INSERT INTO precomputed_reports (nonexistent_column)
                        VALUES ('test')
                    """)

                # Should be a MySQL error about unknown column
                assert "Unknown column" in str(exc_info.value) or "nonexistent_column" in str(exc_info.value)
